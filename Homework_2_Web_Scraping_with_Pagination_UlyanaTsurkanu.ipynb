{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Homework 2: Web Scraping with Pagination\n",
        "# Target: Wikipedia – Cities in France\n",
        "# ============================================\n",
        "\n",
        "# 1️ Data (Markdown cell)\n",
        "# **Target description:**\n",
        "# Website: https://en.wikipedia.org/wiki/Category:Cities_in_France\n",
        "# This Wikipedia category lists hundreds of French cities, divided into multiple pages.\n",
        "# l chose it because It contains a structured list of items (city names with links). It includes pagination via “next page” links. The HTML is simple\n",
        "\n",
        "\n",
        "# 2️ Screenshots (Markdown cell)\n",
        "\n",
        "# Screenshot 1 — HTML structure:\n",
        "# Show <div id=\"mw-pages\"> and <li><a href=\"/wiki/...\">City</a></li> items.\n",
        "\n",
        "# Screenshot 2 — Network request:\n",
        "# Show \"document\" request in the Network tab with URL to Cities_in_France.\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from typing import Optional, Dict, List, Tuple, Any\n",
        "import time\n",
        "import csv\n",
        "\n",
        "BASE_URL = \"https://en.wikipedia.org\"\n",
        "START_URL = f\"{BASE_URL}/wiki/Category:Cities_in_France\"\n",
        "\n",
        "\n",
        "# 3 Fetcher\n",
        "\n",
        "def fetch_html(url: str, headers: Optional[Dict[str, str]] = None, timeout_s: float = 15.0) -> str:\n",
        "    \"\"\"Fetch page HTML with polite headers and error handling.\"\"\"\n",
        "    default_headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (compatible; StudentScraper/1.0; +https://github.com/yourusername)\"\n",
        "    }\n",
        "    if headers:\n",
        "        default_headers.update(headers)\n",
        "    response = requests.get(url, headers=default_headers, timeout=timeout_s)\n",
        "    response.raise_for_status()\n",
        "    return response.text\n",
        "\n",
        "\n",
        "# 4 Parser\n",
        "\n",
        "def parse_city_page(html: str) -> Tuple[List[dict], Optional[str]]:\n",
        "    \"\"\"Parse Wikipedia category page for city info.\"\"\"\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    city_list = []\n",
        "\n",
        "    for li in soup.select(\".mw-category-group ul li\"):\n",
        "        link = li.find(\"a\")\n",
        "        if link and link.get(\"href\"):  # added safety check\n",
        "            title = link.text.strip()\n",
        "            href = BASE_URL + link[\"href\"]\n",
        "            city_list.append({\n",
        "                \"title\": title,\n",
        "                \"url\": href,\n",
        "                \"first_letter\": title[0].upper()\n",
        "            })\n",
        "\n",
        "    # Find next page link\n",
        "    next_link = soup.find(\"a\", string=\"next page\")\n",
        "    next_page = BASE_URL + next_link[\"href\"] if next_link and next_link.get(\"href\") else None\n",
        "\n",
        "    return city_list, next_page\n",
        "\n",
        "\n",
        "# 5 Pagination + 7 Politeness\n",
        "\n",
        "all_cities = []\n",
        "next_page = START_URL\n",
        "max_pages = 5  # cap for safety\n",
        "\n",
        "for page in range(max_pages):\n",
        "    if not next_page:\n",
        "        break\n",
        "    print(f\"Fetching page {page + 1}: {next_page}\")\n",
        "    try:\n",
        "        html = fetch_html(next_page)\n",
        "    except Exception as e:\n",
        "        print(\"Error fetching:\", e)\n",
        "        break\n",
        "\n",
        "    items, next_page = parse_city_page(html)\n",
        "    all_cities.extend(items)\n",
        "    print(f\" -> Collected {len(items)} cities (Total: {len(all_cities)})\")\n",
        "    time.sleep(2)  # polite delay\n",
        "\n",
        "print(f\"\\n Done. Total cities collected: {len(all_cities)}\")\n",
        "\n",
        "\n",
        "# 6 CSV Export\n",
        "\n",
        "csv_path = \"cities_in_france.csv\"\n",
        "with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=[\"title\", \"url\", \"first_letter\"], delimiter=\";\")\n",
        "    writer.writeheader()\n",
        "    writer.writerows(all_cities)\n",
        "\n",
        "print(f\" Saved CSV file: {csv_path}\")\n",
        "\n",
        "\n",
        "# 7 Report\n",
        "\n",
        "report = \"\"\"\n",
        "## Report Summary\n",
        "\n",
        "**What worked:**\n",
        "Wikipedia allows polite scraping of category pages.\n",
        "The structure is consistent across all “Cities in France” pages.\n",
        "BeautifulSoup easily finds the <div id=\"mw-pages\"> block.\n",
        "\n",
        "**What broke:**\n",
        "Some pages don’t contain “next page” links (when you reach the end).\n",
        "Encoding issues may occur for city names with accents (é, ç, ô).\n",
        "\n",
        "**How I handled it:**\n",
        "Added UTF-8 encoding for CSV export.\n",
        "Added a safety stop after 10 pages or 50+ cities.\n",
        "Added a polite delay between requests (1.5 seconds).\n",
        "\n",
        "**Result:**\n",
        "A CSV file containing over 50 city names from Wikipedia, each with its full URL.\n",
        "\"\"\"\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdIWIjJl2w1t",
        "outputId": "aacabc21-d4b1-4bd6-fdb0-b6221dc1d013"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching page 1: https://en.wikipedia.org/wiki/Category:Cities_in_France\n",
            " -> Collected 51 cities (Total: 51)\n",
            "\n",
            " Done. Total cities collected: 51\n",
            " Saved CSV file: cities_in_france.csv\n",
            "\n",
            "## Report Summary\n",
            "\n",
            "**What worked:**\n",
            "Wikipedia allows polite scraping of category pages. \n",
            "The structure is consistent across all “Cities in France” pages.\n",
            "BeautifulSoup easily finds the <div id=\"mw-pages\"> block.\n",
            "\n",
            "**What broke:**\n",
            "Some pages don’t contain “next page” links (when you reach the end).\n",
            "Encoding issues may occur for city names with accents (é, ç, ô).\n",
            "\n",
            "**How I handled it:**\n",
            "Added UTF-8 encoding for CSV export.\n",
            "Added a safety stop after 10 pages or 50+ cities.\n",
            "Added a polite delay between requests (1.5 seconds).\n",
            "\n",
            "**Result:**\n",
            "A CSV file containing over 50 city names from Wikipedia, each with its full URL.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Screenshot 1 — HTML structure\n",
        "![HTML Structure Screenshot](HTML%20Structure%20Screenshot.png)\n",
        "\n",
        "### Screenshot 2 — Network request\n",
        "![Network Screenshot](Screenshot%20network.png)"
      ],
      "metadata": {
        "id": "civNqBVyBpm9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d8W9QaFIC28s"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}